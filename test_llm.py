#!/usr/bin/env python3
"""
LLM API Test Script
"""

import requests
import json
import time


def test_llm_endpoint():
    """Test the LLM endpoint with OpenAI"""

    url = "http://localhost:8000/llm"

    # Test data
    test_requests = [
        {"prompt": "Merhaba! NasÄ±lsÄ±n?", "llm_provider": "openai"},
        {
            "prompt": "Python hakkÄ±nda kÄ±sa bir bilgi ver",
            "llm_provider": "openai",
            "temperature": 0.5,
            "max_tokens": 150,
        },
        {
            "prompt": "Ä°stanbul'un tarihi hakkÄ±nda 2 cÃ¼mle yaz",
            "llm_provider": "openai",
            "temperature": 0.7,
        },
    ]

    print("ğŸš€ LLM API Test BaÅŸlÄ±yor...")
    print("=" * 50)

    for i, test_data in enumerate(test_requests, 1):
        print(f"\nğŸ“¤ Test {i}: {test_data['prompt'][:30]}...")
        print(f"   Provider: {test_data['llm_provider']}")

        try:
            # Send request
            response = requests.post(
                url,
                json=test_data,
                headers={"Content-Type": "application/json"},
                timeout=30,
            )

            print(f"   Status: {response.status_code}")

            if response.status_code == 200:
                data = response.json()
                print(f"âœ… BaÅŸarÄ±lÄ±!")
                print(f"   ğŸ“¨ Cevap: {data['response'][:100]}...")
                print(f"   ğŸ”§ Provider: {data['provider']}")
                if "model" in data and data["model"]:
                    print(f"   ğŸ¤– Model: {data['model']}")
                if "usage" in data and data["usage"]:
                    print(f"   ğŸ“Š Usage: {data['usage']}")
            else:
                print(f"âŒ Hata: {response.status_code}")
                try:
                    error_data = response.json()
                    print(f"   Detay: {error_data.get('detail', 'Bilinmeyen hata')}")
                except:
                    print(f"   Raw response: {response.text}")

        except requests.exceptions.ConnectionError:
            print("âŒ BaÄŸlantÄ± hatasÄ± - Server Ã§alÄ±ÅŸÄ±yor mu?")
            break
        except requests.exceptions.Timeout:
            print("âŒ Timeout hatasÄ± - Request Ã§ok uzun sÃ¼rdÃ¼")
        except Exception as e:
            print(f"âŒ Beklenmeyen hata: {e}")

        # Wait between requests
        if i < len(test_requests):
            print("   â³ 2 saniye bekleniyor...")
            time.sleep(2)

    print("\n" + "=" * 50)
    print("ğŸ Test tamamlandÄ±!")


def test_health():
    """Test health endpoint first"""
    print("ğŸ” Health check yapÄ±lÄ±yor...")
    try:
        response = requests.get("http://localhost:8000/health", timeout=5)
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… Server saÄŸlÄ±klÄ± - Status: {data['status']}")
            print(f"   Aktif sessionlar: {data['active_sessions']}")
            return True
        else:
            print(f"âŒ Health check baÅŸarÄ±sÄ±z: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Health check hatasÄ±: {e}")
        return False


def main():
    print("ğŸ¯ AI Gateway LLM Test Suite")
    print("=" * 50)

    # Wait for server to be ready
    print("â³ Server'Ä±n hazÄ±r olmasÄ± bekleniyor...")
    time.sleep(3)

    # Health check first
    if not test_health():
        print("âŒ Server'a baÄŸlanÄ±lamÄ±yor. main.py Ã§alÄ±ÅŸÄ±yor mu?")
        return

    print()
    # Run LLM tests
    test_llm_endpoint()


if __name__ == "__main__":
    main()
